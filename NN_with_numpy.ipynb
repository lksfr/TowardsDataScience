{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to utilize the neural network project structure proposed by Andrew Ng in his deeplearning.ai Deep Learning Specialization in order to try to code one of the assignments of the course myself to gain an even deeper understanding. I have changed certain parts to make sure that this code does not provide solutions to the assignment but purely serves its self-study purpose.\n",
    "\n",
    "If you'd like to learn more about deep learning, I'd highly recommend Andrew Ng's courses on Coursera: https://www.deeplearning.ai/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define the neural network structure ( # of input units,  # of hidden units, etc). \n",
    "2. Initialize the model's parameters\n",
    "3. Loop:\n",
    "    - Implement forward propagation\n",
    "    - Compute loss\n",
    "    - Implement backward propagation to get the gradients\n",
    "    - Update parameters (gradient descent)\n",
    "You often build helper functions to compute steps 1-3 and then merge them into one function we call nn_model(). Once you've built nn_model() and learnt the right parameters, you can make predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the only package we are going to use\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Defining Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_structure(x, y, hidden_size):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        x: predictors\n",
    "        y: response variable\n",
    "        hidden_size: number of neurons in the hidden layer\n",
    "        \n",
    "    Output:\n",
    "        input_size: number of predictors in input dataset\n",
    "        hidden_size: number of neurosn in hidden layer\n",
    "        output_size: number of possible prediction outputs  \n",
    "    \"\"\"\n",
    "    input_size = x.shape[0]\n",
    "    hidden_size = hidden_size\n",
    "    output_size = y.shape[0]\n",
    "    \n",
    "    return (input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initializing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(input_size, hidden_size, output_size):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        Sizes of input, hidden, and output layer\n",
    "    Output:\n",
    "        \n",
    "    \"\"\"\n",
    "    W_1 = np.random.randn(hidden_size, input_size) * 0.001\n",
    "    b_1 = np.zeros((hidden_size, 1))\n",
    "    W_2 = np.random.randn(output_size, hidden_size) * 0.001\n",
    "    b_2 = np.zeros((output_size, 1))\n",
    "    \n",
    "    parameters = {'W_1':W_1,\n",
    "                  'b_1':b_1,\n",
    "                  'W_2':W_2,\n",
    "                  'b_2':b_2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Forward Propagation***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a sigmoid and tanh function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    \"\"\"\n",
    "    Inputs z and outputs the tanh of z\n",
    "    \"\"\"\n",
    "    \n",
    "    t = (np.exp(z)-np.exp(-z))/(np.exp(z)+np.exp(-z))\n",
    "    \n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Inputs z and outputs the sigmoid of z\n",
    "    \"\"\"\n",
    "    \n",
    "    s = 1/(1+np.exp(-z))\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the input and the initialized parameters to compute the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(x, parameters):\n",
    "    \"\"\"\n",
    "     Taking the input X along with the parameters and computing the output                                             \n",
    "    \"\"\"\n",
    "    \n",
    "    #retrieving the initialized parameters from the 'parameters' dictionary\n",
    "    W_1, b_1, W_2, b_2 = parameters['W_1'], parameters['b_1'], parameters['W_2'], parameters['b_2']\n",
    "    \n",
    "    #computing the linear and activation part of both the hidden and the output layer\n",
    "    Z_1 = np.dot(W_1, x) + b_1\n",
    "    A_1 = tanh(Z_1)\n",
    "    Z_2 = np.dot(W_2, A_1) + b_2\n",
    "    A_2 = sigmoid(Z_2)\n",
    "    \n",
    "    #storing results in new dictionary called 'cache'\n",
    "    cache = {'Z1': Z_1,\n",
    "             'A1': A_1,\n",
    "             'Z2': Z_2,\n",
    "             'A2': A_2}\n",
    "    \n",
    "    return cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step: Computing the loss of our output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(A2, y, parameters):\n",
    "    \"\"\"\n",
    "    Computing the cross-entropy loss\n",
    "    \"\"\"\n",
    "    \n",
    "    n_observations = y.shape[1]\n",
    "    \n",
    "    loss = 1/n_observations * np.sum(y*np.log(A2) + (1-y)*np.log(1-A2))\n",
    "    \n",
    "    loss = np.squeeze(loss)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the derivatives in backprop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(parameters, cache, x, y):\n",
    "    \"\"\"\n",
    "    Taking the derivatives in backprop\n",
    "    \"\"\"\n",
    "    \n",
    "    m = x.shape[1] #number of observations\n",
    "    \n",
    "    #retrieving parameters as well as calculated output from forward propagation\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    \n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "    \n",
    "    #calculating the derivatives\n",
    "    dZ2 = A2 - y\n",
    "    dW2 = 1/m * np.dot(dZ2, A1.T)\n",
    "    db2 = 1/m * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2))\n",
    "    dW1 = 1/m * np.dot(dZ1, x.T)\n",
    "    db1 = 1/m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating parameters with our previously computed derivatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate = 1):\n",
    "    \"\"\"\n",
    "    Updating the parameters after backprop\n",
    "    \"\"\"\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "    \n",
    "    W1 = W1 - learning_rate*dW1\n",
    "    b1 = b1 - learning_rate*db1\n",
    "    W2 = W2 - learning_rate*dW2\n",
    "    b2 = b2 - learning_rate*db2\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting everything together into a single function representing the neural net:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net(x, y, hidden_size, iterations = 1000):\n",
    "    \"\"\"\n",
    "    Putting all functions together to form a neural network\n",
    "    \"\"\"\n",
    "    input_size = layer_sizes(x, y)[0]\n",
    "    output_size = layer_sizes(x, y)[2]\n",
    "    \n",
    "    parameters = initialize_parameters(input_size, hidden_size, output_size)\n",
    "    W1 = parameters[\"W_1\"]\n",
    "    b1 = parameters[\"b_1\"]\n",
    "    W2 = parameters[\"W_2\"]\n",
    "    b2 = parameters[\"b_2\"]\n",
    "    \n",
    "    for i in range(0, num_iterations): #gradient descent\n",
    "         \n",
    "        A2, cache = forward_propagation(x, parameters) #forward prop\n",
    "        \n",
    "        cost = compute_cost(A2, y, parameters) #computing the cost of our output\n",
    " \n",
    "        grads = backpropagation(parameters, cache, x, y) #taking the derivatives\n",
    " \n",
    "        parameters = update_parameters(parameters, grads) #updating our parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
